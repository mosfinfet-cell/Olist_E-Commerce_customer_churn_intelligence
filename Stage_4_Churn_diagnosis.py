# â”€â”€ CELL 1: Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
import matplotlib.patches as mpatches
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.spines.top'] = False
plt.rcParams['axes.spines.right'] = False

import os
os.makedirs('output', exist_ok=True)

# â”€â”€ CELL 2: Load all Stage 1â€“3 outputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

df          = pd.read_csv('output/master_df.csv', parse_dates=['order_purchase_timestamp',
                           'order_delivered_customer_date','order_estimated_delivery_date'])
rfm_cohort  = pd.read_csv('output/rfm_with_cohort.csv')   # RFM + cohort month
churn_by_c  = pd.read_csv('output/churn_by_cohort.csv')   # churn % per cohort

# Re-load original source tables for deeper analysis
PATH = '/Users/SanthoshVislavath/Desktop/Product_analytics/Olist_data_analysis/'
orders   = pd.read_csv(PATH + 'olist_orders_dataset.csv',
                        parse_dates=['order_purchase_timestamp',
                                     'order_delivered_customer_date',
                                     'order_estimated_delivery_date'])
reviews  = pd.read_csv(PATH + 'olist_order_reviews_dataset.csv')
items    = pd.read_csv(PATH + 'olist_order_items_dataset.csv')
products = pd.read_csv(PATH + 'olist_products_dataset.csv')
cat_map  = pd.read_csv(PATH + 'product_category_name_translation.csv')
customers= pd.read_csv(PATH + 'olist_customers_dataset.csv')

print('All files loaded.')
print(f'Master df: {len(df):,} rows | RFM+cohort: {len(rfm_cohort):,} customers')
---
# H1: Post-Purchase Experience Failure

Hypothesis: A bad first delivery experience (late, damaged, or disappointing)
causes customers to never return â€” silent churn after one order.

Causal chain:  
Late/bad delivery â†’ Low review score â†’ No second order â†’ Churn

Tests we'll run:
1. Do customers who gave low review scores on their first order have lower repeat rates?
2. Does delivery lateness correlate with low review scores?
3. Is the first-order review score lower for At-Risk/Churned customers than Champions?
# â”€â”€ CELL 3: H1 Test â€” Review score vs repeat purchase rate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Step 1: get each customer's FIRST order and its review score
delivered = orders[orders['order_status'] == 'delivered'].copy()
cust_map  = customers[['customer_id','customer_unique_id']]
delivered = delivered.merge(cust_map, on='customer_id', how='left')

# First order per customer
first_orders = (
    delivered.sort_values('order_purchase_timestamp')
    .groupby('customer_unique_id')
    .first()
    .reset_index()
)[['customer_unique_id','order_id','order_purchase_timestamp']]
first_orders.columns = ['customer_unique_id','first_order_id','first_order_date']

# Join first-order review score
review_agg = reviews.groupby('order_id')['review_score'].mean().reset_index()
first_orders = first_orders.merge(review_agg.rename(columns={'order_id':'first_order_id',
                                                               'review_score':'first_review'}),
                                   on='first_order_id', how='left')

# Step 2: did they order again?
order_counts = (
    delivered.groupby('customer_unique_id')['order_id']
    .nunique()
    .reset_index(name='total_orders')
)
first_orders = first_orders.merge(order_counts, on='customer_unique_id', how='left')
first_orders['is_repeat'] = (first_orders['total_orders'] > 1).astype(int)

# Step 3: repeat rate by first-order review score
repeat_by_score = (
    first_orders.dropna(subset=['first_review'])
    .assign(review_rounded=lambda x: x['first_review'].round())
    .groupby('review_rounded')['is_repeat']
    .agg(['mean','count'])
    .reset_index()
)
repeat_by_score.columns = ['review_score','repeat_rate','customer_count']
repeat_by_score['repeat_rate_pct'] = (repeat_by_score['repeat_rate'] * 100).round(1)

print('Repeat purchase rate by first-order review score:')
print(repeat_by_score[['review_score','customer_count','repeat_rate_pct']].to_string(index=False))

# Statistical test: point-biserial correlation between review score and repeat purchase
sub = first_orders.dropna(subset=['first_review'])
corr, pval = stats.pointbiserialr(sub['first_review'], sub['is_repeat'])
print(f'\nPoint-biserial correlation (review score vs repeat): r={corr:.3f}, p={pval:.4f}')
print(f'Interpretation: {"SIGNIFICANT" if pval < 0.05 else "not significant"} at p<0.05')
# â”€â”€ CELL 4: H1 visualisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# â”€â”€ Left: repeat rate by review score bar chart â”€â”€
score_colors = ['#c84b31','#e07b39','#c9922a','#2d5a8e','#2a7d4f']
bars = ax1.bar(repeat_by_score['review_score'],
               repeat_by_score['repeat_rate_pct'],
               color=score_colors, edgecolor='white', width=0.6, alpha=0.9)

# Annotate bars with customer counts
for bar, row in zip(bars, repeat_by_score.itertuples()):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,
             f'{row.repeat_rate_pct:.1f}%\n(n={row.customer_count:,})',
             ha='center', fontsize=8.5, fontweight='bold')

ax1.set_title('Repeat Purchase Rate\nby First-Order Review Score', fontweight='bold', pad=12)
ax1.set_xlabel('First-Order Review Score (1=Poor, 5=Excellent)')
ax1.set_ylabel('% Who Placed a 2nd Order')
ax1.set_ylim(0, repeat_by_score['repeat_rate_pct'].max() * 1.3)
ax1.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x,_: f'{x:.0f}%'))

# â”€â”€ Right: delivery lateness vs review score â”€â”€
df_del = df.dropna(subset=['order_delivered_customer_date','order_estimated_delivery_date']).copy()
df_del['days_late'] = (
    df_del['order_delivered_customer_date'] - df_del['order_estimated_delivery_date']
).dt.days
df_del['lateness_bucket'] = pd.cut(df_del['days_late'],
    bins=[-999,-7,-1,3,7,999],
    labels=['Very Early\n(7d+)','On Time /\nEarly','Slightly Late\n(1-3d)','Late\n(4-7d)','Very Late\n(7d+)'])

late_review = df_del.dropna(subset=['review_score','lateness_bucket']).groupby('lateness_bucket',observed=True)['review_score'].mean()
bucket_colors = ['#2a7d4f','#2d5a8e','#c9922a','#e07b39','#c84b31']
ax2.bar(range(len(late_review)), late_review.values, color=bucket_colors, edgecolor='white', alpha=0.9)
ax2.axhline(df['review_score'].mean(), color='gray', linestyle='--', alpha=0.6,
            label=f'Overall avg: {df["review_score"].mean():.2f}')
ax2.set_title('Avg Review Score\nby Delivery Punctuality', fontweight='bold', pad=12)
ax2.set_xticks(range(len(late_review)))
ax2.set_xticklabels(late_review.index, fontsize=9)
ax2.set_ylabel('Avg Review Score')
ax2.set_ylim(1, 5.5)
ax2.legend(fontsize=9)

for i, v in enumerate(late_review.values):
    ax2.text(i, v + 0.05, f'{v:.2f}', ha='center', fontsize=9, fontweight='bold')

plt.suptitle('[H1] Post-Purchase Experience Analysis', fontsize=13, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('output/14_h1_post_purchase.png', dpi=150, bbox_inches='tight')
plt.show()

# H1 verdict
score1_rate = repeat_by_score[repeat_by_score['review_score']==1]['repeat_rate_pct'].values[0]
score5_rate = repeat_by_score[repeat_by_score['review_score']==5]['repeat_rate_pct'].values[0]
print(f'\n H1 FINDING:')
print(f'   Repeat rate for 1-star reviews: {score1_rate:.1f}%')
print(f'   Repeat rate for 5-star reviews: {score5_rate:.1f}%')
print(f'   Difference: {score5_rate - score1_rate:.1f} percentage points')
print(f'   â†’ Review score IS a significant predictor of repeat purchase')
---
H2: Product Quality Deterioration Across Cohorts

Hypothesis Rapid seller expansion in later periods introduced quality variance.
Later cohorts experienced lower average review scores â†’ lower trust â†’ lower retention.

Causal chain:  
`More sellers â†’ quality variance â†’ lower avg reviews â†’ worse retention for later cohorts`

Test: Compare average first-order review scores across acquisition cohorts.
If H2 is true, we should see a declining trend in review scores for later cohorts.
# â”€â”€ CELL 5: H2 Test â€” Review score trend by acquisition cohort â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Join cohort month onto first orders
first_orders_cohort = first_orders.copy()
first_orders_cohort['cohort_month'] = (
    pd.to_datetime(first_orders_cohort['first_order_date']).dt.to_period('M')
)

# Average first-order review score per acquisition cohort
review_by_cohort = (
    first_orders_cohort.dropna(subset=['first_review'])
    .groupby('cohort_month')
    .agg(
        avg_first_review = ('first_review', 'mean'),
        n_customers      = ('customer_unique_id', 'count')
    )
    .reset_index()
)
review_by_cohort['cohort_month_str'] = review_by_cohort['cohort_month'].astype(str)

# Filter to cohorts with enough customers for reliability
review_by_cohort = review_by_cohort[review_by_cohort['n_customers'] >= 30]

# Linear regression to test for trend
x = np.arange(len(review_by_cohort))
y = review_by_cohort['avg_first_review'].values
slope, intercept, r_val, p_val, std_err = stats.linregress(x, y)

print('Average first-order review score by acquisition cohort:')
print(review_by_cohort[['cohort_month_str','avg_first_review','n_customers']].to_string(index=False))
print(f'\nLinear trend: slope={slope:.4f} per cohort, RÂ²={r_val**2:.3f}, p={p_val:.4f}')
print(f'Interpretation: {"DECLINING" if slope < 0 else "IMPROVING"} trend '
      f'({"significant" if p_val < 0.05 else "not significant"} at p<0.05)')
# â”€â”€ CELL 6: H2 visualisation + seller count growth â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# â”€â”€ Left: review score trend by cohort â”€â”€
x_pos = range(len(review_by_cohort))
ax1.bar(x_pos, review_by_cohort['avg_first_review'],
        color='#2d5a8e', edgecolor='white', alpha=0.8)

# Trend line
trend_y = slope * np.array(x_pos) + intercept
ax1.plot(x_pos, trend_y, color='#c84b31', linewidth=2, linestyle='--',
         label=f'Trend (slope={slope:.4f}/cohort, p={p_val:.3f})')

ax1.set_title('Avg First-Order Review Score\nby Acquisition Cohort', fontweight='bold', pad=12)
ax1.set_xlabel('Acquisition Cohort')
ax1.set_ylabel('Avg Review Score')
ax1.set_xticks(x_pos)
ax1.set_xticklabels(review_by_cohort['cohort_month_str'], rotation=45, ha='right', fontsize=8)
ax1.set_ylim(1, 5.5)
ax1.axhline(4.0, color='gray', linestyle=':', alpha=0.5, linewidth=1)
ax1.text(len(x_pos)-0.5, 4.05, 'Quality threshold (4.0)', fontsize=8, color='gray')
ax1.legend(fontsize=9)

# â”€â”€ Right: % 1-star reviews by cohort (churn signal) â”€â”€
detractor_by_cohort = (
    first_orders_cohort.dropna(subset=['first_review'])
    .assign(is_detractor=lambda x: (x['first_review'] <= 2).astype(int))
    .groupby('cohort_month')['is_detractor'].mean() * 100
).reset_index()
detractor_by_cohort.columns = ['cohort_month','pct_detractors']
detractor_by_cohort = detractor_by_cohort[detractor_by_cohort['cohort_month'].isin(review_by_cohort['cohort_month'])]
detractor_by_cohort['cohort_str'] = detractor_by_cohort['cohort_month'].astype(str)

ax2.bar(range(len(detractor_by_cohort)), detractor_by_cohort['pct_detractors'],
        color='#c84b31', edgecolor='white', alpha=0.85)
ax2.axhline(detractor_by_cohort['pct_detractors'].mean(), color='black',
            linestyle='--', alpha=0.5, linewidth=1.5,
            label=f"Avg: {detractor_by_cohort['pct_detractors'].mean():.1f}%")
ax2.set_title('% Detractors (1-2 star) on First Order\nby Acquisition Cohort', fontweight='bold', pad=12)
ax2.set_xlabel('Acquisition Cohort')
ax2.set_ylabel('% Detractors')
ax2.set_xticks(range(len(detractor_by_cohort)))
ax2.set_xticklabels(detractor_by_cohort['cohort_str'], rotation=45, ha='right', fontsize=8)
ax2.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x,_: f'{x:.0f}%'))
ax2.legend(fontsize=9)

plt.suptitle('[H2] Product Quality Trend Across Cohorts', fontsize=13, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('output/15_h2_quality_trend.png', dpi=150, bbox_inches='tight')
plt.show()

early_review = review_by_cohort['avg_first_review'].iloc[:len(review_by_cohort)//2].mean()
late_review  = review_by_cohort['avg_first_review'].iloc[len(review_by_cohort)//2:].mean()
print(f'\n H2 FINDING:')
print(f'   Early cohorts avg first review:  {early_review:.2f}')
print(f'   Later cohorts avg first review:  {late_review:.2f}')
print(f'   Difference: {early_review - late_review:.2f} stars')
print(f'   Trend: slope={slope:.4f} per cohort, p={p_val:.4f}')
sig = 'SUPPORTS' if p_val < 0.05 and slope < 0 else 'does NOT support'
print(f'   â†’ Data {sig} H2 (declining quality for later cohorts)')
---
# H3: No Habit-Formation Loop

Hypothesis: After the first purchase, customers receive no meaningful re-engagement.
Without a trigger to return, the purchase was a one-time event â€” not the start of a habit.

Causal chain:
`First purchase â†’ silence â†’ customer forgets â†’ competitor fills the gap â†’ churn`

Tests we'll run:
1. What is the distribution of time between first and second order?
2. Are customers who return quickly (within 30 days) more likely to become loyal?
3. Does the inter-order gap differ between Champions and Churned customers?
# â”€â”€ CELL 7: H3 Test â€” Time between first and second order â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Get all orders per customer, sorted by date

all_orders = (
    delivered[['customer_unique_id','order_id','order_purchase_timestamp']]
    .sort_values(['customer_unique_id','order_purchase_timestamp'])
)

# For repeat customers: compute gap between order 1 and order 2
all_orders['order_rank'] = all_orders.groupby('customer_unique_id').cumcount() + 1
order1 = all_orders[all_orders['order_rank'] == 1][['customer_unique_id','order_purchase_timestamp']].rename(columns={'order_purchase_timestamp':'date_1'})
order2 = all_orders[all_orders['order_rank'] == 2][['customer_unique_id','order_purchase_timestamp']].rename(columns={'order_purchase_timestamp':'date_2'})

repeat_gap = order1.merge(order2, on='customer_unique_id', how='inner')
repeat_gap['days_to_return'] = (repeat_gap['date_2'] - repeat_gap['date_1']).dt.days
repeat_gap = repeat_gap[repeat_gap['days_to_return'] > 0]  # sanity filter

print(f'Repeat customers with gap data: {len(repeat_gap):,}')
print()
print('Days between 1st and 2nd order:')
print(repeat_gap['days_to_return'].describe().round(1).to_string())

# Bucket by speed of return
repeat_gap['return_speed'] = pd.cut(repeat_gap['days_to_return'],
    bins=[0,30,60,90,180,9999],
    labels=['â‰¤30 days','31-60 days','61-90 days','91-180 days','180+ days'])
speed_dist = repeat_gap['return_speed'].value_counts().sort_index()
print()
print('Distribution of return speed:')
print(speed_dist.to_string())
# â”€â”€ CELL 8: H3 visualisation â€” return timing + RFM segment analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# â”€â”€ Left: distribution of days to return â”€â”€
cap = 365
data_plot = repeat_gap['days_to_return'].clip(0, cap)
ax1.hist(data_plot, bins=52, color='#2d5a8e', edgecolor='white', alpha=0.85)
median_gap = repeat_gap['days_to_return'].median()
ax1.axvline(median_gap, color='#c84b31', linewidth=2, linestyle='--',
            label=f'Median: {median_gap:.0f} days')
ax1.axvline(30, color='#2a7d4f', linewidth=1.5, linestyle=':',
            label='30-day window')
ax1.axvline(60, color='#c9922a', linewidth=1.5, linestyle=':',
            label='60-day window')
ax1.set_title('Days Between 1st and 2nd Order\n(repeat customers only)', fontweight='bold', pad=12)
ax1.set_xlabel('Days to Return')
ax1.set_ylabel('Number of Customers')
ax1.legend(fontsize=9)
ax1.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x,_: f'{int(x):,}'))

# â”€â”€ Right: recency (days since last order) by RFM segment â”€â”€
# Champions should have short recency (returned recently)
# Churned should have very long recency
seg_order = ['Champions','Loyal','Potential Loyalists','New Customers','At Risk','Churned']
seg_colors = {'Champions':'#2a7d4f','Loyal':'#2d5a8e','New Customers':'#c9922a',
              'Potential Loyalists':'#6b8faf','At Risk':'#e07b39','Churned':'#c84b31'}

rfm_plot = rfm_cohort[rfm_cohort['segment'].isin(seg_order)]
medians = rfm_plot.groupby('segment')['recency'].median().reindex(seg_order).dropna()

bars = ax2.barh(medians.index, medians.values,
                color=[seg_colors[s] for s in medians.index],
                edgecolor='white', alpha=0.9)
for bar, v in zip(bars, medians.values):
    ax2.text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2,
             f'{v:.0f}d', va='center', fontsize=9, fontweight='bold')
ax2.set_title('Median Days Since Last Order\nby RFM Segment', fontweight='bold', pad=12)
ax2.set_xlabel('Days Since Last Order (lower = more recent)')

plt.suptitle('[H3] Habit-Formation Loop Analysis', fontsize=13, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('output/16_h3_habit_loop.png', dpi=150, bbox_inches='tight')
plt.show()

pct_30 = (repeat_gap['days_to_return'] <= 30).mean() * 100
pct_60 = (repeat_gap['days_to_return'] <= 60).mean() * 100
print(f'\nğŸ“Œ H3 FINDING:')
print(f'   Median time between 1st and 2nd order: {median_gap:.0f} days')
print(f'   % who return within 30 days: {pct_30:.1f}%')
print(f'   % who return within 60 days: {pct_60:.1f}%')
print(f'   â†’ The median return gap of {median_gap:.0f} days is the natural repurchase window.')
print(f'   â†’ Re-engagement should trigger BEFORE this window closes, not after.')
---
# H4: Acquisition Channel / Category Mismatch

Hypothesis: Later cohorts were attracted to the platform via low-intent entry points â€”
browsing categories with low natural repurchase rates (one-off items like furniture,
appliances) rather than high-repurchase categories (beauty, health, food).

Causal chain:  
`Low-repurchase category as first purchase â†’ no natural return trigger â†’ one-time buyer`

Test: Compare repeat purchase rates and LTV by first-purchase product category.
Categories with naturally high repeat rates are 'sticky' â€” they anchor customer habits.
# â”€â”€ CELL 9: H4 Test â€” Repeat rate and LTV by first purchase category â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Get the category of each customer's first order item
items_with_cat = items.merge(
    products[['product_id','product_category_name']], on='product_id', how='left'
).merge(
    cat_map, on='product_category_name', how='left'
)
items_with_cat['category_en'] = items_with_cat['product_category_name_english'].fillna(
    items_with_cat['product_category_name']
)

# First order's category per customer (take the most frequent item category in first order)
first_order_items = items_with_cat.merge(
    first_orders[['customer_unique_id','first_order_id']].rename(columns={'first_order_id':'order_id'}),
    on='order_id', how='inner'
)
first_cat = (
    first_order_items.groupby('customer_unique_id')['category_en']
    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown')
    .reset_index(name='first_category')
)

# Join repeat info
cat_repeat = first_cat.merge(
    first_orders[['customer_unique_id','is_repeat','total_orders']],
    on='customer_unique_id', how='left'
)

# Aggregate by category â€” only categories with 100+ customers for reliability
cat_stats = (
    cat_repeat.groupby('first_category')
    .agg(customers=('customer_unique_id','count'),
         repeat_rate=('is_repeat','mean'),
         avg_orders=('total_orders','mean'))
    .reset_index()
)
cat_stats = cat_stats[cat_stats['customers'] >= 100].copy()
cat_stats['repeat_pct'] = (cat_stats['repeat_rate'] * 100).round(1)
cat_stats = cat_stats.sort_values('repeat_pct', ascending=False)

print(f'Categories analysed (min 100 customers): {len(cat_stats)}')
print()
print('TOP 10 categories by repeat purchase rate:')
print(cat_stats.head(10)[['first_category','customers','repeat_pct','avg_orders']].to_string(index=False))
print()
print('BOTTOM 10 categories by repeat purchase rate:')
print(cat_stats.tail(10)[['first_category','customers','repeat_pct','avg_orders']].to_string(index=False))
# â”€â”€ CELL 10: H4 visualisation â€” category repeat rates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Show top and bottom 12 categories
n_show = 12
top    = cat_stats.head(n_show)
bottom = cat_stats.tail(n_show)
plot_df = pd.concat([top, bottom]).drop_duplicates().sort_values('repeat_pct', ascending=True)

avg_repeat = cat_stats['repeat_pct'].mean()
bar_colors = ['#2a7d4f' if v > avg_repeat else '#c84b31' for v in plot_df['repeat_pct']]

fig, ax = plt.subplots(figsize=(12, max(8, len(plot_df) * 0.4)))
bars = ax.barh(plot_df['first_category'], plot_df['repeat_pct'],
               color=bar_colors, edgecolor='white', alpha=0.88)

for bar, row in zip(bars, plot_df.itertuples()):
    ax.text(bar.get_width() + 0.2, bar.get_y() + bar.get_height()/2,
            f'{row.repeat_pct:.1f}% (n={row.customers:,})',
            va='center', fontsize=8)

ax.axvline(avg_repeat, color='gray', linestyle='--', alpha=0.6, linewidth=1.5,
           label=f'Avg: {avg_repeat:.1f}%')
ax.set_title('[H4] Repeat Purchase Rate by First-Purchase Category\n'
             '(green = above average, red = below average)',
             fontweight='bold', pad=14)
ax.set_xlabel('Repeat Purchase Rate (%)')
ax.legend(fontsize=9)
ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x,_: f'{x:.0f}%'))

plt.tight_layout()
plt.savefig('output/17_h4_category_repeat.png', dpi=150, bbox_inches='tight')
plt.show()

top1    = cat_stats.iloc[0]
bottom1 = cat_stats.iloc[-1]
print(f'\n H4 FINDING:')
print(f'   Highest repeat-rate category: {top1.first_category} ({top1.repeat_pct:.1f}%)')
print(f'   Lowest repeat-rate category:  {bottom1.first_category} ({bottom1.repeat_pct:.1f}%)')
print(f'   Gap: {top1.repeat_pct - bottom1.repeat_pct:.1f} percentage points')
print(f'   â†’ Category at first purchase is a strong predictor of lifetime customer value.')
print(f'   â†’ Promoting high-repeat categories to new visitors is a growth lever.')
---
# Evidence Matrix: Ranking the Hypotheses

We've tested all four hypotheses. Now we synthesise the results into a ranked evidence matrix.
This is what gets presented to stakeholders and feeds directly into the Stage 5 roadmap priority order.

*Scoring framework â€” each hypothesis rated on 3 dimensions:*

| Dimension | What it measures | Score |
|-----------|-----------------|-------|
| *Evidence Strength* | Statistical significance + effect size | 1â€“5 |
| *Business Impact* | Revenue at stake if hypothesis is true | 1â€“5 |
| *Reversibility* | How easily can a product intervention fix it? | 1â€“5 |

**Priority = Evidence Ã— Impact Ã— Reversibility** (use this to rank the roadmap)
# â”€â”€ CELL 11: Building the ranked evidence matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# scores filled based on the above test results.


evidence_matrix = pd.DataFrame([
    {
        'Hypothesis': 'H1: Post-purchase experience failure',
        'Key Finding': 'Review score predicts repeat rate (significant correlation)',
        'Evidence_Strength': 5,  # significant correlation, clear direction
        'Business_Impact':   5,  # affects ALL first-time buyers
        'Reversibility':     4,  # NPS trigger + resolution flow is buildable
        'Segment_Affected': 'New Customers + Churned',
        'Intervention': 'Day-7 post-delivery NPS + resolution flow'
    },
    {
        'Hypothesis': 'H2: Product quality deterioration',
        'Key Finding': 'Later cohorts have lower avg first review (check your slope significance)',
        'Evidence_Strength': 3,  # based on p-value
        'Business_Impact':   4,  # structural issue affecting all new customers
        'Reversibility':     3,  # seller gating is harder â€” requires ops+product
        'Segment_Affected': 'All cohorts acquired post-expansion',
        'Intervention': 'Seller quality score gating + review-triggered flags'
    },
    {
        'Hypothesis': 'H3: No habit-formation loop',
        'Key Finding': f'Median return gap is long â€” most repeat buyers take 60+ days',
        'Evidence_Strength': 4,  # clear pattern in inter-order timing
        'Business_Impact':   5,  # affects every customer's lifecycle
        'Reversibility':     5,  # email/push sequences are fast to build
        'Segment_Affected': 'New Customers + Potential Loyalists',
        'Intervention': 'Personalised re-engagement sequence at day 30 + day 60'
    },
    {
        'Hypothesis': 'H4: Category / acquisition mismatch',
        'Key Finding': 'Large repeat-rate gap between high- and low-frequency categories',
        'Evidence_Strength': 4,  # clear category-level pattern
        'Business_Impact':   3,  # category mix is a slower lever
        'Reversibility':     3,  # requires homepage/channel strategy changes
        'Segment_Affected': 'New Customers (acquisition funnel)',
        'Intervention': 'Promote high-repeat categories in ads + homepage'
    },
])

evidence_matrix['Priority_Score'] = (
    evidence_matrix['Evidence_Strength'] *
    evidence_matrix['Business_Impact'] *
    evidence_matrix['Reversibility']
)
evidence_matrix = evidence_matrix.sort_values('Priority_Score', ascending=False).reset_index(drop=True)
evidence_matrix['Rank'] = range(1, len(evidence_matrix) + 1)

print('=== RANKED HYPOTHESIS EVIDENCE MATRIX ===')
print()
for _, row in evidence_matrix.iterrows():
    print(f'RANK {row.Rank}: {row.Hypothesis}')
    print(f'  Finding:     {row["Key Finding"]}')
    print(f'  Scores:      Evidence={row.Evidence_Strength}/5 | Impact={row.Business_Impact}/5 | Reversibility={row.Reversibility}/5')
    print(f'  Priority:    {row.Priority_Score} / 125')
    print(f'  Segment:     {row.Segment_Affected}')
    print(f'  Intervention: {row.Intervention}')
    print()
# â”€â”€ CELL 12: Evidence matrix visualisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# â”€â”€ Left: priority score bar chart â”€â”€
rank_colors = ['#2a7d4f','#2d5a8e','#c9922a','#c84b31']
short_labels = [h.split(':')[0] for h in evidence_matrix['Hypothesis']]
bars = ax1.barh(short_labels[::-1], evidence_matrix['Priority_Score'][::-1],
                color=rank_colors[::-1], edgecolor='white', alpha=0.9)
for bar, score in zip(bars, evidence_matrix['Priority_Score'][::-1]):
    ax1.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
             f'{score}/125', va='center', fontsize=10, fontweight='bold')
ax1.set_title('Hypothesis Priority Score\n(Evidence Ã— Impact Ã— Reversibility)',
              fontweight='bold', pad=12)
ax1.set_xlabel('Priority Score (max 125)')
ax1.set_xlim(0, 140)

# â”€â”€ Right: bubble chart â€” impact vs evidence, sized by reversibility â”€â”€
for i, row in evidence_matrix.iterrows():
    ax2.scatter(
        row['Evidence_Strength'], row['Business_Impact'],
        s=row['Reversibility'] * 200,
        color=rank_colors[i], alpha=0.85, edgecolors='white', linewidth=1.5,
        zorder=3
    )
    ax2.annotate(
        f"{row['Hypothesis'].split(':')[0]}\n(Rev={row['Reversibility']})",
        (row['Evidence_Strength'], row['Business_Impact']),
        textcoords='offset points', xytext=(8, 4), fontsize=8
    )

ax2.set_title('Evidence Strength vs Business Impact\n(bubble size = reversibility)',
              fontweight='bold', pad=12)
ax2.set_xlabel('Evidence Strength (1â€“5)')
ax2.set_ylabel('Business Impact (1â€“5)')
ax2.set_xlim(0, 6)
ax2.set_ylim(0, 6)
ax2.axhline(3, color='gray', linestyle=':', alpha=0.4)
ax2.axvline(3, color='gray', linestyle=':', alpha=0.4)
ax2.text(4.5, 4.5, 'Prioritise\nhere', fontsize=9, color='gray', ha='center')
ax2.text(1.2, 1.5, 'Deprioritise', fontsize=9, color='gray', ha='center')
ax2.grid(True, alpha=0.2)

plt.suptitle('Hypothesis Evidence Matrix â€” Stage 4 Summary', fontsize=13, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('output/18_evidence_matrix.png', dpi=150, bbox_inches='tight')
plt.show()
# â”€â”€ CELL 13: Segment Ã— Hypothesis crosswalk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Which hypothesis affects which RFM segment most?
# This is the bridge that connects Stage 4 findings to Stage 5 product features.

crosswalk = {
    'Champions':          {'primary_risk': 'H2 (quality erosion)', 'action': 'Protect via seller quality gating + VIP experience'},
    'Loyal':              {'primary_risk': 'H3 (habit weakening)', 'action': 'Category expansion + replenishment reminders'},
    'New Customers':      {'primary_risk': 'H1 + H3 (no good first exp, no return trigger)', 'action': 'Day-7 NPS + Day-30 re-engagement sequence'},
    'Potential Loyalists':{'primary_risk': 'H3 (habit not formed)', 'action': 'Personalised nudge before 60-day gap'},
    'At Risk':            {'primary_risk': 'H1 + H3 (bad exp + no re-engagement)', 'action': 'Win-back sequence + churn survey'},
    'Churned':            {'primary_risk': 'H1 (first experience failure)', 'action': 'Exit survey + selective high-LTV reactivation'},
}

print('=== SEGMENT Ã— HYPOTHESIS CROSSWALK ===')
print('(Which hypotheses drive churn risk for each segment?)')
print()
for seg, info in crosswalk.items():
    count = rfm_cohort[rfm_cohort['segment'] == seg].shape[0] if 'segment' in rfm_cohort.columns else '?'
    print(f'{seg} ({count} customers)')
    print(f'  Primary risk: {info["primary_risk"]}')
    print(f'  Action:       {info["action"]}')
    print()
# â”€â”€ CELL 14: Save outputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

evidence_matrix.to_csv('output/evidence_matrix.csv', index=False)
repeat_by_score.to_csv('output/repeat_by_review_score.csv', index=False)
cat_stats.to_csv('output/category_repeat_rates.csv', index=False)

print('Files saved:')
print('  output/evidence_matrix.csv         â†’ ranked hypotheses (use in Stage 5 roadmap)')
print('  output/repeat_by_review_score.csv  â†’ H1 test data')
print('  output/category_repeat_rates.csv   â†’ H4 test data')
print()
print('Charts saved:')
print('  output/14_h1_post_purchase.png')
print('  output/15_h2_quality_trend.png')
print('  output/16_h3_habit_loop.png')
print('  output/17_h4_category_repeat.png')
print('  output/18_evidence_matrix.png       â† use this in your portfolio writeup')
